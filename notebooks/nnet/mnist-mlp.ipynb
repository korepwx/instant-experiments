{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ipwx/anaconda/envs/tensorflow/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from ipwxlearn import glue\n",
    "from ipwxlearn.datasets import mnist\n",
    "from ipwxlearn.glue import G\n",
    "from ipwxlearn.utils import dataflow, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    cache_dir = os.path.abspath('../../data')\n",
    "    train_X, train_y, test_X, test_y = mnist.read_data_sets(cache_dir=cache_dir, floatX=glue.config.floatX)\n",
    "\n",
    "    # split train-test set.\n",
    "    indices = np.arange(train_X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    valid_size = int(train_X.shape[0] * 0.1)\n",
    "    train_idx, valid_idx = indices[:-valid_size], indices[-valid_size:]\n",
    "    return (train_X[train_idx], train_y[train_idx]), (train_X[valid_idx], train_y[valid_idx]), \\\n",
    "           (test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_X, train_y), (valid_X, valid_y), (test_X, test_y) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the multilayer perceptron.\n",
    "graph = G.Graph()\n",
    "with graph.as_default():\n",
    "    target_num = 10\n",
    "    train_input_shape = (32, 784)\n",
    "    test_input_shape = (None,) + train_input_shape[1:]\n",
    "\n",
    "    train_input = G.make_placeholder('trainX', shape=train_input_shape, dtype=glue.config.floatX)\n",
    "    train_label = G.make_placeholder('trainY', shape=train_input_shape[:1], dtype=np.int32)\n",
    "    test_input = G.make_placeholder('testX', shape=test_input_shape, dtype=glue.config.floatX)\n",
    "    test_label = G.make_placeholder('testY', shape=test_input_shape[:1], dtype=np.int32)\n",
    "\n",
    "    # compose the network\n",
    "    input = G.layers.InputLayer(train_input, shape=train_input_shape)\n",
    "    dropout0 = G.layers.DropoutLayer('dropout0', input, p=0.2)\n",
    "    hidden1 = G.layers.DenseLayer('hidden1', dropout0, num_units=128)\n",
    "    dropout1 = G.layers.DropoutLayer('dropout1', hidden1, p=0.5)\n",
    "    hidden2 = G.layers.DenseLayer('hidden2', dropout1, num_units=32)\n",
    "    dropout2 = G.layers.DropoutLayer('dropout2', hidden1, p=0.5)\n",
    "    softmax = G.layers.SoftmaxLayer('softmax', dropout2, num_units=target_num)\n",
    "\n",
    "    # derivate the predictions and loss\n",
    "    train_output, train_loss = G.layers.get_output_with_sparse_softmax_crossentropy(softmax, train_label)\n",
    "    train_loss = G.op.mean(train_loss)\n",
    "\n",
    "    test_output, test_loss = G.layers.get_output_with_sparse_softmax_crossentropy(\n",
    "        softmax, test_label, inputs={input: test_input}, deterministic=True)\n",
    "    test_loss = G.op.sum(test_loss)\n",
    "    test_predict = G.op.argmax(test_output, axis=1)\n",
    "\n",
    "    # Create update expressions for training.\n",
    "    params = G.layers.get_all_params(softmax, trainable=True)\n",
    "    updates = G.updates.adam(train_loss, params)\n",
    "\n",
    "    train_fn = G.make_function(inputs=[train_input, train_label], outputs=train_loss, updates=updates)\n",
    "    valid_fn = G.make_function(inputs=[test_input, test_label], outputs=test_loss)\n",
    "    test_fn = G.make_function(inputs=[test_input], outputs=test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: train loss 0.404123, valid loss 0.418383 (*)\n",
      "Step 400: train loss 0.615130, valid loss 0.328621 (*)\n",
      "Step 600: train loss 0.437812, valid loss 0.292462 (*)\n",
      "Step 800: train loss 0.307806, valid loss 0.266479 (*)\n",
      "Step 1000: train loss 0.554026, valid loss 0.241796 (*)\n",
      "Step 1200: train loss 0.337766, valid loss 0.227426 (*)\n",
      "Step 1400: train loss 0.420344, valid loss 0.213586 (*)\n",
      "Step 1600: train loss 0.416756, valid loss 0.202593 (*)\n",
      "Step 1800: train loss 0.238488, valid loss 0.189669 (*)\n",
      "Step 2000: train loss 0.115722, valid loss 0.181842 (*)\n",
      "Step 2200: train loss 0.137478, valid loss 0.178297 (*)\n",
      "Step 2400: train loss 0.351777, valid loss 0.165907 (*)\n",
      "Step 2600: train loss 0.202765, valid loss 0.163965 (*)\n",
      "Step 2800: train loss 0.481491, valid loss 0.158473 (*)\n",
      "Step 3000: train loss 0.214683, valid loss 0.150889 (*)\n",
      "Step 3200: train loss 0.307927, valid loss 0.149114 (*)\n",
      "Step 3400: train loss 0.190956, valid loss 0.143436 (*)\n",
      "Step 3600: train loss 0.153051, valid loss 0.140957 (*)\n",
      "Step 3800: train loss 0.256378, valid loss 0.144139\n",
      "Step 4000: train loss 0.136192, valid loss 0.138644 (*)\n",
      "Step 4200: train loss 0.524341, valid loss 0.135344 (*)\n",
      "Step 4400: train loss 0.380754, valid loss 0.129676 (*)\n",
      "Step 4600: train loss 0.264960, valid loss 0.135324\n",
      "Step 4800: train loss 0.478875, valid loss 0.125076 (*)\n",
      "Step 5000: train loss 0.138127, valid loss 0.123673 (*)\n",
      "Step 5200: train loss 0.089763, valid loss 0.124186\n",
      "Step 5400: train loss 0.405391, valid loss 0.118875 (*)\n",
      "Step 5600: train loss 0.324160, valid loss 0.125472\n",
      "Step 5800: train loss 0.138309, valid loss 0.117820 (*)\n",
      "Step 6000: train loss 0.160560, valid loss 0.120355\n",
      "Step 6200: train loss 0.446482, valid loss 0.118459\n",
      "Step 6400: train loss 0.162078, valid loss 0.112175 (*)\n",
      "Step 6600: train loss 0.245334, valid loss 0.112886\n",
      "Step 6800: train loss 0.067549, valid loss 0.112313\n",
      "Step 7000: train loss 0.456836, valid loss 0.113036\n",
      "Step 7200: train loss 0.151744, valid loss 0.106428 (*)\n",
      "Step 7400: train loss 0.105114, valid loss 0.109263\n",
      "Step 7600: train loss 0.473105, valid loss 0.111340\n",
      "Step 7800: train loss 0.024891, valid loss 0.107971\n",
      "Step 8000: train loss 0.395227, valid loss 0.105613 (*)\n",
      "Step 8200: train loss 0.294587, valid loss 0.106644\n",
      "Step 8400: train loss 0.392247, valid loss 0.105543 (*)\n",
      "Step 8600: train loss 0.293217, valid loss 0.107167\n",
      "Step 8800: train loss 0.233314, valid loss 0.103852 (*)\n",
      "Step 9000: train loss 0.086708, valid loss 0.101521 (*)\n",
      "Step 9200: train loss 0.156525, valid loss 0.103717\n",
      "Step 9400: train loss 0.188244, valid loss 0.102289\n",
      "Step 9600: train loss 0.085428, valid loss 0.104423\n",
      "Step 9800: train loss 0.122015, valid loss 0.104478\n",
      "Step 10000: train loss 0.159279, valid loss 0.104235\n",
      "Step 10200: train loss 0.064644, valid loss 0.098767 (*)\n",
      "Step 10400: train loss 0.202071, valid loss 0.103162\n",
      "Step 10600: train loss 0.111987, valid loss 0.103036\n",
      "Step 10800: train loss 0.038792, valid loss 0.097171 (*)\n",
      "Step 11000: train loss 0.102251, valid loss 0.100916\n",
      "Step 11200: train loss 0.271664, valid loss 0.098006\n",
      "Step 11400: train loss 0.256848, valid loss 0.096203 (*)\n",
      "Step 11600: train loss 0.050154, valid loss 0.098567\n",
      "Step 11800: train loss 0.255915, valid loss 0.095605 (*)\n",
      "Step 12000: train loss 0.270484, valid loss 0.097365\n",
      "Step 12200: train loss 0.033965, valid loss 0.092858 (*)\n",
      "Step 12400: train loss 0.034346, valid loss 0.092905\n",
      "Step 12600: train loss 0.173389, valid loss 0.093801\n",
      "Step 12800: train loss 0.152044, valid loss 0.095648\n",
      "Step 13000: train loss 0.351852, valid loss 0.093455\n",
      "Step 13200: train loss 0.021770, valid loss 0.097190\n",
      "Step 13400: train loss 0.106266, valid loss 0.093576\n",
      "Step 13600: train loss 0.025683, valid loss 0.089459 (*)\n",
      "Step 13800: train loss 0.255881, valid loss 0.090938\n",
      "Step 14000: train loss 0.021320, valid loss 0.090992\n",
      "Step 14200: train loss 0.314299, valid loss 0.090348\n",
      "Step 14400: train loss 0.236225, valid loss 0.093796\n",
      "Step 14600: train loss 0.190480, valid loss 0.093484\n",
      "Step 14800: train loss 0.074517, valid loss 0.090888\n",
      "Step 15000: train loss 0.154194, valid loss 0.092320\n",
      "Step 15200: train loss 0.147417, valid loss 0.094588\n",
      "Step 15400: train loss 0.151563, valid loss 0.090980\n",
      "Step 15600: train loss 0.263245, valid loss 0.088631 (*)\n",
      "Step 15800: train loss 0.153230, valid loss 0.093559\n",
      "Step 16000: train loss 0.057251, valid loss 0.089792\n",
      "Step 16200: train loss 0.315216, valid loss 0.087081 (*)\n",
      "Step 16400: train loss 0.160085, valid loss 0.088539\n",
      "Step 16600: train loss 0.117588, valid loss 0.088454\n",
      "Step 16800: train loss 0.075285, valid loss 0.088066\n"
     ]
    }
   ],
   "source": [
    "# train the MLP.\n",
    "with G.Session(graph) as sess:\n",
    "    monitors = training.ValidationMonitor(valid_fn, (valid_X, valid_y), params=params, log_file=sys.stdout)\n",
    "    training.run_steps(train_fn, (train_X, train_y), monitor=monitors, max_steps=10 * len(train_X) // 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 2.60 %\n"
     ]
    }
   ],
   "source": [
    "with G.Session(graph) as sess:\n",
    "    # After training, we compute and print the test error.\n",
    "    test_predicts = []\n",
    "    for test_batch_X in dataflow.iterate_testing_batches(test_X, batch_size=256):\n",
    "        test_predicts.append(test_fn(test_batch_X))\n",
    "    test_predicts = np.concatenate(test_predicts, axis=0).astype(np.int32)\n",
    "    print('Test error: %.2f %%' % (float(np.mean(test_predicts != test_y)) * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
