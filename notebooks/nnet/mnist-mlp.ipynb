{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ipwx/anaconda/envs/tensorflow/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from ipwxlearn import glue\n",
    "from ipwxlearn.datasets import mnist\n",
    "from ipwxlearn.glue import G\n",
    "from ipwxlearn.utils import dataflow, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    cache_dir = os.path.abspath('../../data')\n",
    "    train_X, train_y, test_X, test_y = mnist.read_data_sets(cache_dir=cache_dir, floatX=glue.config.floatX)\n",
    "\n",
    "    # split train-test set.\n",
    "    indices = np.arange(train_X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    valid_size = int(train_X.shape[0] * 0.1)\n",
    "    train_idx, valid_idx = indices[:-valid_size], indices[-valid_size:]\n",
    "    return (train_X[train_idx], train_y[train_idx]), (train_X[valid_idx], train_y[valid_idx]), \\\n",
    "           (test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_X, train_y), (valid_X, valid_y), (test_X, test_y) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the multilayer perceptron.\n",
    "graph = G.Graph()\n",
    "with graph.as_default():\n",
    "    target_num = 10\n",
    "    train_input_shape = (32, 784)\n",
    "    test_input_shape = (None,) + train_input_shape[1:]\n",
    "\n",
    "    train_input = G.make_placeholder('trainX', shape=train_input_shape, dtype=glue.config.floatX)\n",
    "    train_label = G.make_placeholder('trainY', shape=train_input_shape[:1], dtype=np.int32)\n",
    "    test_input = G.make_placeholder('testX', shape=test_input_shape, dtype=glue.config.floatX)\n",
    "    test_label = G.make_placeholder('testY', shape=test_input_shape[:1], dtype=np.int32)\n",
    "\n",
    "    # compose the network\n",
    "    input = G.layers.InputLayer(train_input, shape=train_input_shape)\n",
    "    dropout0 = G.layers.DropoutLayer('dropout0', input, p=0.2)\n",
    "    hidden1 = G.layers.DenseLayer('hidden1', dropout0, num_units=128)\n",
    "    dropout1 = G.layers.DropoutLayer('dropout1', hidden1, p=0.5)\n",
    "    hidden2 = G.layers.DenseLayer('hidden2', dropout1, num_units=32)\n",
    "    dropout2 = G.layers.DropoutLayer('dropout2', hidden1, p=0.5)\n",
    "    softmax = G.layers.SoftmaxLayer('softmax', dropout2, num_units=target_num)\n",
    "\n",
    "    # derivate the predictions and loss\n",
    "    train_output = G.layers.get_output(softmax)\n",
    "    train_loss = G.op.mean(G.objectives.sparse_categorical_crossentropy(train_output, train_label))\n",
    "\n",
    "    test_output = G.layers.get_output(softmax, inputs={input: test_input}, deterministic=True)\n",
    "    test_loss = G.op.sum(G.objectives.sparse_categorical_crossentropy(test_output, test_label))\n",
    "    test_predict = G.op.argmax(test_output, axis=1)\n",
    "\n",
    "    # Create update expressions for training.\n",
    "    params = G.layers.get_all_params(softmax, trainable=True)\n",
    "    updates = G.updates.adam(train_loss, params)\n",
    "\n",
    "    train_fn = G.make_function(inputs=[train_input, train_label], outputs=train_loss, updates=updates)\n",
    "    valid_fn = G.make_function(inputs=[test_input, test_label], outputs=test_loss)\n",
    "    test_fn = G.make_function(inputs=[test_input], outputs=test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: train loss 0.665509, valid loss 0.450832 (*)\n",
      "Step 400: train loss 0.385806, valid loss 0.333764 (*)\n",
      "Step 600: train loss 0.755397, valid loss 0.294522 (*)\n",
      "Step 800: train loss 0.337349, valid loss 0.269586 (*)\n",
      "Step 1000: train loss 0.241877, valid loss 0.249626 (*)\n",
      "Step 1200: train loss 0.424761, valid loss 0.228390 (*)\n",
      "Step 1400: train loss 0.236672, valid loss 0.219577 (*)\n",
      "Step 1600: train loss 0.203284, valid loss 0.205687 (*)\n",
      "Step 1800: train loss 0.233868, valid loss 0.196585 (*)\n",
      "Step 2000: train loss 0.296461, valid loss 0.189495 (*)\n",
      "Step 2200: train loss 0.171812, valid loss 0.177086 (*)\n",
      "Step 2400: train loss 0.221327, valid loss 0.174106 (*)\n",
      "Step 2600: train loss 0.212054, valid loss 0.172257 (*)\n",
      "Step 2800: train loss 0.223170, valid loss 0.160464 (*)\n",
      "Step 3000: train loss 0.543407, valid loss 0.160148 (*)\n",
      "Step 3200: train loss 0.138891, valid loss 0.164002\n",
      "Step 3400: train loss 0.126966, valid loss 0.155478 (*)\n",
      "Step 3600: train loss 0.334295, valid loss 0.149152 (*)\n",
      "Step 3800: train loss 0.459959, valid loss 0.146007 (*)\n",
      "Step 4000: train loss 0.135866, valid loss 0.144227 (*)\n",
      "Step 4200: train loss 0.107394, valid loss 0.136676 (*)\n",
      "Step 4400: train loss 0.203709, valid loss 0.135694 (*)\n",
      "Step 4600: train loss 0.133009, valid loss 0.136149\n",
      "Step 4800: train loss 0.108653, valid loss 0.132714 (*)\n",
      "Step 5000: train loss 0.415098, valid loss 0.130858 (*)\n",
      "Step 5200: train loss 0.044363, valid loss 0.130676 (*)\n",
      "Step 5400: train loss 0.176607, valid loss 0.129472 (*)\n",
      "Step 5600: train loss 0.179855, valid loss 0.125777 (*)\n",
      "Step 5800: train loss 0.136380, valid loss 0.123021 (*)\n",
      "Step 6000: train loss 0.152492, valid loss 0.120717 (*)\n",
      "Step 6200: train loss 0.406692, valid loss 0.120889\n",
      "Step 6400: train loss 0.249781, valid loss 0.120966\n",
      "Step 6600: train loss 0.082123, valid loss 0.120028 (*)\n",
      "Step 6800: train loss 0.028290, valid loss 0.116867 (*)\n",
      "Step 7000: train loss 0.133868, valid loss 0.117029\n",
      "Step 7200: train loss 0.107292, valid loss 0.116960\n",
      "Step 7400: train loss 0.172540, valid loss 0.116604 (*)\n",
      "Step 7600: train loss 0.323351, valid loss 0.112188 (*)\n",
      "Step 7800: train loss 0.149273, valid loss 0.111130 (*)\n",
      "Step 8000: train loss 0.179059, valid loss 0.108494 (*)\n",
      "Step 8200: train loss 0.311913, valid loss 0.115041\n",
      "Step 8400: train loss 0.514651, valid loss 0.111480\n",
      "Step 8600: train loss 0.092695, valid loss 0.110114\n",
      "Step 8800: train loss 0.034069, valid loss 0.107771 (*)\n",
      "Step 9000: train loss 0.441753, valid loss 0.104144 (*)\n",
      "Step 9200: train loss 0.081300, valid loss 0.104102 (*)\n",
      "Step 9400: train loss 0.134399, valid loss 0.101802 (*)\n",
      "Step 9600: train loss 0.035680, valid loss 0.105445\n",
      "Step 9800: train loss 0.084892, valid loss 0.106369\n",
      "Step 10000: train loss 0.093362, valid loss 0.107058\n",
      "Step 10200: train loss 0.287501, valid loss 0.105513\n",
      "Step 10400: train loss 0.145815, valid loss 0.103605\n",
      "Step 10600: train loss 0.363814, valid loss 0.108154\n",
      "Step 10800: train loss 0.157586, valid loss 0.103494\n",
      "Step 11000: train loss 0.176003, valid loss 0.103693\n",
      "Step 11200: train loss 0.116253, valid loss 0.102198\n",
      "Step 11400: train loss 0.542884, valid loss 0.098712 (*)\n",
      "Step 11600: train loss 0.176189, valid loss 0.099747\n",
      "Step 11800: train loss 0.284655, valid loss 0.101712\n",
      "Step 12000: train loss 0.092892, valid loss 0.099580\n",
      "Step 12200: train loss 0.085316, valid loss 0.098989\n",
      "Step 12400: train loss 0.149177, valid loss 0.100267\n",
      "Step 12600: train loss 0.079167, valid loss 0.099686\n",
      "Step 12800: train loss 0.047353, valid loss 0.098974\n",
      "Step 13000: train loss 0.295721, valid loss 0.096714 (*)\n",
      "Step 13200: train loss 0.481138, valid loss 0.097332\n",
      "Step 13400: train loss 0.012351, valid loss 0.098593\n",
      "Step 13600: train loss 0.113134, valid loss 0.101235\n",
      "Step 13800: train loss 0.115005, valid loss 0.096741\n",
      "Step 14000: train loss 0.170510, valid loss 0.096989\n",
      "Step 14200: train loss 0.098503, valid loss 0.092250 (*)\n",
      "Step 14400: train loss 0.484596, valid loss 0.095445\n",
      "Step 14600: train loss 0.117301, valid loss 0.096892\n",
      "Step 14800: train loss 0.129225, valid loss 0.099445\n",
      "Step 15000: train loss 0.376497, valid loss 0.100631\n",
      "Step 15200: train loss 0.104382, valid loss 0.098600\n",
      "Step 15400: train loss 0.584317, valid loss 0.098789\n",
      "Step 15600: train loss 0.045267, valid loss 0.094099\n",
      "Step 15800: train loss 0.041428, valid loss 0.093004\n",
      "Step 16000: train loss 0.188035, valid loss 0.095019\n",
      "Step 16200: train loss 0.056710, valid loss 0.095056\n",
      "Step 16400: train loss 0.123970, valid loss 0.095938\n",
      "Step 16600: train loss 0.148910, valid loss 0.096316\n",
      "Step 16800: train loss 0.072750, valid loss 0.097120\n"
     ]
    }
   ],
   "source": [
    "# train the MLP.\n",
    "with G.Session(graph) as sess:\n",
    "    monitors = training.ValidationMonitor(valid_fn, (valid_X, valid_y), params=params, log_file=sys.stdout)\n",
    "    training.run_steps(train_fn, (train_X, train_y), monitor=monitors, max_steps=10 * len(train_X) // 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 2.54 %\n"
     ]
    }
   ],
   "source": [
    "with G.Session(graph) as sess:\n",
    "    # After training, we compute and print the test error.\n",
    "    test_predicts = []\n",
    "    for test_batch_X in dataflow.iterate_testing_batches(test_X, batch_size=256):\n",
    "        test_predicts.append(test_fn(test_batch_X))\n",
    "    test_predicts = np.concatenate(test_predicts, axis=0).astype(np.int32)\n",
    "    print('Test error: %.2f %%' % (float(np.mean(test_predicts != test_y)) * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
