{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ipwx/projects/ipwxlearn/ipwxlearn/glue/config.py:28: UserWarning: TensorFlow currently lack a good support for float64, downgrade to float32.\n",
      "  warnings.warn('TensorFlow currently lack a good support for float64, downgrade to float32.')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from ipwxlearn import glue\n",
    "from ipwxlearn.datasets import mnist\n",
    "from ipwxlearn.glue import G\n",
    "from ipwxlearn.utils import dataflow, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    cache_dir = os.path.abspath('../../data')\n",
    "    train_X, train_y, test_X, test_y = mnist.read_data_sets(cache_dir=cache_dir, floatX=glue.config.floatX)\n",
    "\n",
    "    # split train-test set.\n",
    "    indices = np.arange(train_X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    valid_size = int(train_X.shape[0] * 0.1)\n",
    "    train_idx, valid_idx = indices[:-valid_size], indices[-valid_size:]\n",
    "    return (train_X[train_idx], train_y[train_idx]), (train_X[valid_idx], train_y[valid_idx]), \\\n",
    "           (test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_X, train_y), (valid_X, valid_y), (test_X, test_y) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the multilayer perceptron.\n",
    "graph = G.Graph()\n",
    "with graph.as_default():\n",
    "    target_num = 10\n",
    "    train_input_shape = (32, 784)\n",
    "    test_input_shape = (None,) + train_input_shape[1:]\n",
    "\n",
    "    train_input = G.make_placeholder('trainX', shape=train_input_shape, dtype=glue.config.floatX)\n",
    "    train_label = G.make_placeholder('trainY', shape=train_input_shape[:1], dtype=np.int32)\n",
    "    test_input = G.make_placeholder('testX', shape=test_input_shape, dtype=glue.config.floatX)\n",
    "    test_label = G.make_placeholder('testY', shape=test_input_shape[:1], dtype=np.int32)\n",
    "\n",
    "    # compose the network\n",
    "    input = G.layers.InputLayer(train_input, shape=train_input_shape)\n",
    "    dropout0 = G.layers.DropoutLayer('dropout0', input, p=0.2)\n",
    "    hidden1 = G.layers.DenseLayer('hidden1', dropout0, num_units=128)\n",
    "    dropout1 = G.layers.DropoutLayer('dropout1', hidden1, p=0.5)\n",
    "    hidden2 = G.layers.DenseLayer('hidden2', dropout1, num_units=32)\n",
    "    dropout2 = G.layers.DropoutLayer('dropout2', hidden1, p=0.5)\n",
    "    softmax = G.layers.SoftmaxLayer('softmax', dropout2, num_units=target_num)\n",
    "\n",
    "    # derivate the predictions and loss\n",
    "    train_output, train_loss = G.layers.get_output_with_sparse_softmax_crossentropy(softmax, train_label)\n",
    "    train_loss = G.op.mean(train_loss)\n",
    "\n",
    "    test_output, test_loss = G.layers.get_output_with_sparse_softmax_crossentropy(\n",
    "        softmax, test_label, inputs={input: test_input}, deterministic=True)\n",
    "    test_loss = G.op.sum(test_loss)\n",
    "    test_predict = G.op.argmax(test_output, axis=1)\n",
    "\n",
    "    # Create update expressions for training.\n",
    "    params = G.layers.get_all_params(softmax, trainable=True)\n",
    "    updates = G.updates.adam(train_loss, params)\n",
    "\n",
    "    train_fn = G.make_function(inputs=[train_input, train_label], outputs=train_loss, updates=updates)\n",
    "    valid_fn = G.make_function(inputs=[test_input, test_label], outputs=test_loss)\n",
    "    test_fn = G.make_function(inputs=[test_input], outputs=test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: train loss 0.831907, valid loss 0.441795 (*)\n",
      "Step 400: train loss 0.675407, valid loss 0.322011 (*)\n",
      "Step 600: train loss 0.591215, valid loss 0.279194 (*)\n",
      "Step 800: train loss 0.755028, valid loss 0.247013 (*)\n",
      "Step 1000: train loss 0.208422, valid loss 0.233136 (*)\n",
      "Step 1200: train loss 0.286936, valid loss 0.210358 (*)\n",
      "Step 1400: train loss 0.255494, valid loss 0.201905 (*)\n",
      "Step 1600: train loss 0.442057, valid loss 0.193467 (*)\n",
      "Step 1800: train loss 0.404126, valid loss 0.185743 (*)\n",
      "Step 2000: train loss 0.457888, valid loss 0.175036 (*)\n",
      "Step 2200: train loss 0.117919, valid loss 0.165635 (*)\n",
      "Step 2400: train loss 0.137376, valid loss 0.164586 (*)\n",
      "Step 2600: train loss 0.075165, valid loss 0.155140 (*)\n",
      "Step 2800: train loss 0.281966, valid loss 0.151200 (*)\n",
      "Step 3000: train loss 0.412106, valid loss 0.151013 (*)\n",
      "Step 3200: train loss 0.225448, valid loss 0.148153 (*)\n",
      "Step 3400: train loss 0.311459, valid loss 0.138837 (*)\n",
      "Step 3600: train loss 0.136715, valid loss 0.137067 (*)\n",
      "Step 3800: train loss 0.173122, valid loss 0.131871 (*)\n",
      "Step 4000: train loss 0.099773, valid loss 0.132956\n",
      "Step 4200: train loss 0.210295, valid loss 0.132892\n",
      "Step 4400: train loss 0.162546, valid loss 0.128663 (*)\n",
      "Step 4600: train loss 0.080704, valid loss 0.125038 (*)\n",
      "Step 4800: train loss 0.399165, valid loss 0.128795\n",
      "Step 5000: train loss 0.101544, valid loss 0.123354 (*)\n",
      "Step 5200: train loss 0.336805, valid loss 0.123344 (*)\n",
      "Step 5400: train loss 0.084135, valid loss 0.122147 (*)\n",
      "Step 5600: train loss 0.286716, valid loss 0.120152 (*)\n",
      "Step 5800: train loss 0.390083, valid loss 0.117265 (*)\n",
      "Step 6000: train loss 0.201258, valid loss 0.114474 (*)\n",
      "Step 6200: train loss 0.148969, valid loss 0.110495 (*)\n",
      "Step 6400: train loss 0.078202, valid loss 0.112670\n",
      "Step 6600: train loss 0.174412, valid loss 0.111945\n",
      "Step 6800: train loss 0.044657, valid loss 0.109845 (*)\n",
      "Step 7000: train loss 0.071993, valid loss 0.108612 (*)\n",
      "Step 7200: train loss 0.091904, valid loss 0.111583\n",
      "Step 7400: train loss 0.289957, valid loss 0.110462\n",
      "Step 7600: train loss 0.103845, valid loss 0.108788\n",
      "Step 7800: train loss 0.415458, valid loss 0.102336 (*)\n",
      "Step 8000: train loss 0.069646, valid loss 0.102700\n",
      "Step 8200: train loss 0.164311, valid loss 0.106318\n",
      "Step 8400: train loss 0.175278, valid loss 0.101839 (*)\n",
      "Step 8600: train loss 0.154762, valid loss 0.103272\n",
      "Step 8800: train loss 0.206245, valid loss 0.104050\n",
      "Step 9000: train loss 0.114219, valid loss 0.101905\n",
      "Step 9200: train loss 0.300750, valid loss 0.098989 (*)\n",
      "Step 9400: train loss 0.247496, valid loss 0.097774 (*)\n",
      "Step 9600: train loss 0.260689, valid loss 0.099842\n",
      "Step 9800: train loss 0.062184, valid loss 0.097760 (*)\n",
      "Step 10000: train loss 0.534875, valid loss 0.098991\n",
      "Step 10200: train loss 0.424743, valid loss 0.099407\n",
      "Step 10400: train loss 0.240601, valid loss 0.098609\n",
      "Step 10600: train loss 0.077345, valid loss 0.096387 (*)\n",
      "Step 10800: train loss 0.144701, valid loss 0.095997 (*)\n",
      "Step 11000: train loss 0.158893, valid loss 0.099794\n",
      "Step 11200: train loss 0.124168, valid loss 0.096215\n",
      "Step 11400: train loss 0.284966, valid loss 0.090772 (*)\n",
      "Step 11600: train loss 0.526102, valid loss 0.092038\n",
      "Step 11800: train loss 0.048067, valid loss 0.091386\n",
      "Step 12000: train loss 0.133647, valid loss 0.091321\n",
      "Step 12200: train loss 0.155653, valid loss 0.092967\n",
      "Step 12400: train loss 0.507965, valid loss 0.091909\n",
      "Step 12600: train loss 0.077546, valid loss 0.094522\n",
      "Step 12800: train loss 0.122698, valid loss 0.087948 (*)\n",
      "Step 13000: train loss 0.075399, valid loss 0.088391\n",
      "Step 13200: train loss 0.166240, valid loss 0.086809 (*)\n",
      "Step 13400: train loss 0.085889, valid loss 0.088022\n",
      "Step 13600: train loss 0.081395, valid loss 0.091062\n",
      "Step 13800: train loss 0.067175, valid loss 0.091162\n",
      "Step 14000: train loss 0.176036, valid loss 0.093214\n",
      "Step 14200: train loss 0.247486, valid loss 0.092368\n",
      "Step 14400: train loss 0.113558, valid loss 0.091761\n",
      "Step 14600: train loss 0.028921, valid loss 0.089275\n",
      "Step 14800: train loss 0.198797, valid loss 0.088444\n",
      "Step 15000: train loss 0.046372, valid loss 0.090732\n",
      "Step 15200: train loss 0.043093, valid loss 0.091192\n",
      "Step 15400: train loss 0.102250, valid loss 0.086839\n",
      "Step 15600: train loss 0.396548, valid loss 0.088863\n",
      "Step 15800: train loss 0.321895, valid loss 0.087884\n",
      "Step 16000: train loss 0.155051, valid loss 0.090264\n",
      "Step 16200: train loss 0.284703, valid loss 0.087591\n",
      "Step 16400: train loss 0.058943, valid loss 0.085414 (*)\n",
      "Step 16600: train loss 0.095898, valid loss 0.087042\n",
      "Step 16800: train loss 0.131490, valid loss 0.089477\n"
     ]
    }
   ],
   "source": [
    "# train the MLP.\n",
    "with G.Session(graph) as sess:\n",
    "    monitors = training.ValidationMonitor(valid_fn, (valid_X, valid_y), params=params, log_file=sys.stdout)\n",
    "    training.run_steps(train_fn, (train_X, train_y), monitor=monitors, max_steps=10 * len(train_X) // 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 2.63 %\n"
     ]
    }
   ],
   "source": [
    "with G.Session(graph) as sess:\n",
    "    # After training, we compute and print the test error.\n",
    "    test_predicts = []\n",
    "    for test_batch_X in dataflow.iterate_testing_batches(test_X, batch_size=256):\n",
    "        test_predicts.append(test_fn(test_batch_X))\n",
    "    test_predicts = np.concatenate(test_predicts, axis=0).astype(np.int32)\n",
    "    print('Test error: %.2f %%' % (float(np.mean(test_predicts != test_y)) * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}